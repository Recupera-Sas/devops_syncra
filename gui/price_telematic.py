import pandas as pd
from gui import registers_telematic
import numpy as np
import os
from datetime import datetime

# --- Step 1: Define Key Columns for Each File Type (UNCHANGED) ---
COLUMNS_SMS_SAEM = [
    "pais", "id", "nombre campa√±as", "usuario", "username", "tipo", "flash",
    "fecha inicio", "fecha creaci√≥n", "archivo", "fecha fin", "estados",
    "registros", "cargados", "ejecutados", "aperturas", "respuestas",
    "ejecuci√≥n", "progresivo", "periodo", "tolva", "estado_atr", "rango_validacion"
]

COLUMNS_SMS_SAEM_2 = ['respuesta doble via', 'fecha de creacion', 'enviados', 'registros', 
    'total', 'fecha de finalizaci√≥n', 'codigo estado', 'doble via', 'id', 'flash', 'cr√©ditos', 
    'estado', 'aperuras landing', 'bulk', 'nombre de la campa√±a', 'tipo de campa√±a', 'tipo servicio', 
    'codigo servicio', 'errados', 'id usuario', 'progresivo', 'fecha de inicio', 'periodo', 
    'nombre de usuario', 'pa√≠s'
]

COLUMNS_IVR_SAEM = [
    "pais", "id campa√±a", "nombre campa√±a", "usuario", "id usuario",
    "fecha programada", "fecha registro", "archivo telefonos", "audio",
    "fecha finalizaci√≥n", "estado", "carga", "% ejecucion", "ejecutados",
    "satisfactorios", "colgados", "no contestados", "pendientes", "sin contacto",
    "ultima llamada", "std", "segundos", "repasos"
]

COLUMNS_EMAIL_MASIVIAN = [
    "id cuenta", "cuenta", "campa√±a", "asunto", "fecha de env√≠o", "estado campa√±a",
    "total cargados", "procesados", "no procesados", "no enviados", "% no enviados",
    "entregados", "% entregados", "abiertos", "% abiertos", "clics", "% clics",
    "diferidos", "% diferidos", "spam", "% spam", "dados de baja", "% dados de baja",
    "rebote fuerte", "% rebote fuerte", "rebote suave", "% rebote suave",
    "clics unicos", "% clics unicos", "aperturas unicas", "% aperturas unicas",
    "rechazados", "% rechazados", "adjuntos", "adjuntos genericos", "adjuntos personsalizados",
    "fecha de creaci√≥n", "remitente", "enviado por", "id campa√±a", "correo de aprobaci√≥n",
    "fecha de cancelaci√≥n", "cancelado por", "descripci√≥n", "etiquetas", "cc", "cco"
]

COLUMNS_SMS_MASIVIAN = [
    "packageid", "fecha creacion", "fecha programado", "cliente", "usuario",
    "total registros cargados", "total mensajes programados", "total mensajes erroneos",
    "total mensajes enviados", "es premium", "es flash", "campa√±a", "mensaje",
    "tipo de env√≠o", "descripci√≥n", "total de clicks", "click unicos",
    "total restricciones", "total procesados", "destinatario restringido"
]

COLUMNS_IVR_IPCOM = [
    'dst party id', 'account name', 'costo', 'pa√≠s del c√≥digo', 'src party id', 'subscriber name',
    'tarifa', 'tiempo facturado', 'nombre del c√≥digo', 'leg id', 'connect time'
]


COLUMNS_WISEBOT_BASE = ["campa√±a", "fecha_estado_final", "rut", "telefono", "estado_llamada", "tiempo_llamada"]
COLUMNS_WISEBOT_BASE_2 = ["campa√±a", "fecha_llamada", "rut", "telefono", "estado_llamada", "tiempo_llamada"]
COLUMNS_WISEBOT_BENEFITS = COLUMNS_WISEBOT_BASE + ["nombre", "apellido", "desea_beneficios"]
COLUMNS_WISEBOT_AGREEMENT = COLUMNS_WISEBOT_BASE + ["id base", "fecha_acuerdo", "fecha_plazo"]
COLUMNS_WISEBOT_TITULAR = COLUMNS_WISEBOT_BASE + ["marca"]

# --- Step 2: Column Name Normalization Function (UNCHANGED) ---
def normalize_columns(columns):
    """üìù Normalizes a list of column names (lowercase, no extra spaces)."""
    return [str(col).strip().lower() for col in columns]

# --- Step 3: File Classification Function (UNCHANGED) ---
def classify_excel_file(file_path):
    """
    üè∑Ô∏è Classifies an Excel or CSV file based on its column names.
    üìä Reads all sheets (for Excel) or the single file (for CSV) to consolidate headers.
    """
    # 1. Determine File Type
    file_extension = os.path.splitext(file_path)[1].lower()
    all_headers = set()

    try:
        if file_extension in ('.xls', '.xlsx'):
            # --- üìä Excel File Handling ---
            print(f"   üìë Reading Excel file...")
            xls = pd.ExcelFile(file_path)
            print(f"   üìÑ Sheets found: {len(xls.sheet_names)}")

            for sheet_name in xls.sheet_names:
                # Read only the header row (nrows=0)
                df_temp = xls.parse(sheet_name, nrows=0) 
                normalized_sheet_headers = normalize_columns(df_temp.columns.tolist())
                all_headers.update(normalized_sheet_headers)
        
        elif file_extension == '.csv':
            # --- üìä CSV File Handling ---
            print(f"   üìë Reading CSV file...")
            try:
                df_temp = pd.read_csv(file_path, nrows=0, sep=None, engine='python', encoding='utf-8')
            except UnicodeDecodeError:
                df_temp = pd.read_csv(file_path, nrows=0, sep=None, engine='python', encoding='latin-1')
             
            normalized_file_headers = normalize_columns(df_temp.columns.tolist())
            all_headers.update(normalized_file_headers)

        else:
            print(f"   ‚ùå Error: Unsupported file type: {file_extension}. Must be .xls, .xlsx, or .csv.")
            return "unsupported_type", []

        # 2. Classification Logic (Remains the same)
        present_headers = list(all_headers)
        print(f"   üîç Columns found: {len(present_headers)}")
        
        if all(col in present_headers for col in COLUMNS_WISEBOT_BENEFITS):
            return "wisebot_benefits", present_headers
        elif all(col in present_headers for col in COLUMNS_WISEBOT_AGREEMENT):
            return "wisebot_agreement", present_headers
        elif all(col in present_headers for col in COLUMNS_WISEBOT_TITULAR):
            return "wisebot_titular", present_headers
        elif all(col in present_headers for col in COLUMNS_WISEBOT_BASE):
            return "wisebot_base", present_headers
        elif all(col in present_headers for col in COLUMNS_WISEBOT_BASE_2):
            return "wisebot_base", present_headers
        elif all(col in present_headers for col in COLUMNS_SMS_SAEM_2):
            return "sms_saem", present_headers
        elif all(col in present_headers for col in COLUMNS_IVR_SAEM):
            return "ivr_saem", present_headers
        elif all(col in present_headers for col in COLUMNS_EMAIL_MASIVIAN):
            return "email_masivian", present_headers
        elif all(col in present_headers for col in COLUMNS_SMS_MASIVIAN):
            return "sms_masivian", present_headers
        elif all(col in present_headers for col in COLUMNS_IVR_IPCOM):
            return "ivr_ipcom", present_headers

        print(f"   ‚ö†Ô∏è  Unknown file type")
        return "unknown", present_headers

    except FileNotFoundError:
        print(f"   ‚ùå Error: File '{file_path}' not found.")
        return "file_error", []
    except Exception as e:
        print(f"   ‚ùå Error classifying file '{file_path}': {e}")
        return "classification_error", []

# --- Step 4: Processing Functions for Each Type ---
def _read_and_normalize_excel_data(file_path):
    """Helper function to read all sheets of an Excel and normalize columns."""
    xls = pd.ExcelFile(file_path)
    consolidated_df = pd.DataFrame()
    for sheet_name in xls.sheet_names:
        df_sheet = xls.parse(sheet_name)
        df_sheet.columns = normalize_columns(df_sheet.columns)
        consolidated_df = pd.concat([consolidated_df, df_sheet], ignore_index=True)
    return consolidated_df

# --- Step 4: Processing Functions for Each Type ---
def _read_and_normalize_csv_data(file_path):
    """
    Funci√≥n auxiliar para leer un archivo CSV detectando autom√°ticamente
    el delimitador (',' o ';'), normalizar los nombres de sus columnas 
    y devolver el DataFrame completo.
    """
    # Intentar diferentes encodings y detectar separador autom√°ticamente
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            # Leer solo la primera l√≠nea para detectar separador
            with open(file_path, 'r', encoding=encoding) as f:
                first_line = f.readline()
            
            # Detectar separador (comma vs semicolon)
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            
            # Usar el m√°s frecuente, coma por defecto
            sep = ';' if semicolon_count > comma_count else ','
            
            # Leer el archivo completo
            df = pd.read_csv(file_path, sep=sep, encoding=encoding)
            
            # Normalizar los nombres de las columnas
            df.columns = normalize_columns(df.columns)
            
            return df
            
        except (UnicodeDecodeError, pd.errors.ParserError):
            continue  # Intentar con siguiente encoding
    
    # Si todos los encodings fallan, intentar con detecci√≥n autom√°tica
    try:
        df = pd.read_csv(file_path, sep=None, engine='python', encoding_errors='ignore')
        df.columns = normalize_columns(df.columns)
        return df
    except Exception as e:
        raise ValueError(f"No se pudo leer el archivo {file_path}: {str(e)}")

def _read_and_normalize_csv_data(file_path):
    """
    Funci√≥n auxiliar para leer un archivo CSV detectando autom√°ticamente
    el delimitador (',' o ';'), normalizar los nombres de sus columnas 
    y devolver el DataFrame completo.
    """
    # Intentar diferentes encodings y detectar separador autom√°ticamente
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            # Leer solo la primera l√≠nea para detectar separador
            with open(file_path, 'r', encoding=encoding) as f:
                first_line = f.readline()
            
            # Detectar separador (comma vs semicolon)
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            
            # Usar el m√°s frecuente, coma por defecto
            sep = ';' if semicolon_count > comma_count else ','
            
            # Leer el archivo completo
            df = pd.read_csv(file_path, sep=sep, encoding=encoding)
            
            # Normalizar los nombres de las columnas
            df.columns = normalize_columns(df.columns)
            
            return df
            
        except (UnicodeDecodeError, pd.errors.ParserError):
            continue  # Intentar con siguiente encoding
    
    # Si todos los encodings fallan, intentar con detecci√≥n autom√°tica
    try:
        df = pd.read_csv(file_path, sep=None, engine='python', encoding_errors='ignore')
        df.columns = normalize_columns(df.columns)
        return df
    except Exception as e:
        raise ValueError(f"No se pudo leer el archivo {file_path}: {str(e)}")

# All processing functions now return the processed DataFrame or None
def process_sms_saem(file_path, present_headers):
    """
    üì± Logic to process SMS SAEM files.
    üìà Includes aggregation of 'ejecutados' by 'fecha inicio' (day) and 'username'.
    """
    print(f"üöÄ *** Starting SMS SAEM processing for: '{file_path}' ***")
    df = _read_and_normalize_csv_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    # Add a 'source_file_type' column to identify the data source later in the combined sheet
    df['source_file_type'] = 'SMS_SAEM'

    # --- SMS SAEM SPECIFIC AGGREGATION ---
    if 'enviados' in df.columns and 'fecha de inicio' in df.columns and 'nombre de la campa√±a' in df.columns:
        print("   üìä Performing aggregation for 'enviados' by 'fecha de inicio' and 'nombre de la campa√±a'...")

        # 1. Convert 'enviados' to numeric, filling NaNs with 0
        df['enviados'] = pd.to_numeric(df['enviados'], errors='coerce').fillna(0)
        print("      ‚úÖ 'enviados' column converted to numeric and NaNs filled with 0.")
        # 2. Convert 'fecha de inicio' to datetime and extract the date part
        df['fecha de inicio'] = pd.to_datetime(df['fecha de inicio'], errors='coerce')
        df['fecha_inicio_dia'] = df['fecha de inicio'].dt.floor('D') # Get just the date (YYYY-MM-DD)
        print("      ‚úÖ 'fecha de inicio' converted to datetime and date part extracted.")
        # Filter out rows where 'fecha_inicio_dia' is NaT (invalid date) before grouping
        df_filtered_for_agg = df.dropna(subset=['fecha_inicio_dia'])

        if not df_filtered_for_agg.empty:
            # 3. Group by 'fecha_inicio_dia' and 'nombre de usuario' and sum 'enviados'
            sms_saem_aggregated_df = df_filtered_for_agg.groupby(['fecha_inicio_dia', 'nombre de usuario'])['enviados'].sum().reset_index()
            sms_saem_aggregated_df.rename(columns={'enviados': 'suma_ejecutados_diarios'}, inplace=True)
            sms_saem_aggregated_df['source_file_type'] = 'SMS_SAEM_AGGREGATED'

            print(f"\n   üìà Aggregated SMS SAEM Data: {len(sms_saem_aggregated_df)} rows")

            print(f"üéâ *** SMS SAEM processing finished. Returning original and aggregated data. ***\n" + "-" * 50)
            return [df, sms_saem_aggregated_df] # Return a list of DataFrames
        else:
            print("      ‚ö†Ô∏è  No valid data remaining after filtering for aggregation.")
            print(f"üéâ *** SMS SAEM processing finished. Returning original data only. ***\n" + "-" * 50)
            return df # Return original DataFrame if aggregation failed

    else:
        print("      ‚ö†Ô∏è  Skipping aggregation: 'ejecutados', 'fecha inicio', or 'username' column(s) not found.")

    print(f"üéâ *** SMS SAEM processing finished. ***\n" + "-" * 50)
    return df

def process_ivr_saem(file_path, present_headers):
    """
    üìû Logic to process IVR SAEM files.
    üìà Includes aggregation of 'segundos' by 'fecha programada' (day) and a new
    categorical column derived from 'nombre campa√±a' and the 'Estandar/Personalizado' column.
    """
    print(f"üöÄ *** Starting IVR SAEM processing for: '{file_path}' ***")
    df = _read_and_normalize_excel_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    df['source_file_type'] = 'IVR_SAEM' # Mark the original data

    # --- IVR SAEM SPECIFIC AGGREGATION ---
    required_cols = ['fecha programada', 'segundos', 'nombre campa√±a', 'unnamed: 23']
    # Identify the 'Estandar'/'Personalizado' column (assuming it's a string column
    # that might be unnamed or have a generic name like 'unnamed: x')
    standard_personalizado_col = None
    for col in df.columns:
        # Check if the column contains 'estandar' or 'personalizado' (case-insensitive)
        # and if it's a string type
        if df[col].astype(str).str.contains(r'estandar|personalizado', case=False, na=False).any() and col != 'nombre campa√±a':
            standard_personalizado_col = col
            print(f"   üîç Identified 'Estandar/Personalizado' column as: '{standard_personalizado_col}'")
            required_cols.append(standard_personalizado_col)
            break
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"      ‚ö†Ô∏è  Skipping aggregation: Missing one or more required columns: {missing_cols}.")
        print(f"üéâ *** IVR SAEM processing finished. Returning original data only. ***\n" + "-" * 50)
        return df

    print("   üìä Performing aggregation for 'segundos' by 'fecha programada', 'campaign_group', and 'standard_personalizado_type'...")

    # 1. Convert 'segundos' to numeric, filling NaNs with 0
    df['segundos'] = pd.to_numeric(df['segundos'], errors='coerce').fillna(0)
    print("      ‚úÖ 'segundos' column converted to numeric and NaNs filled with 0.")

    # 2. Convert 'fecha programada' to datetime and extract the date part
    df['fecha programada'] = pd.to_datetime(df['fecha programada'], errors='coerce')
    df['fecha_programada_dia'] = df['fecha programada'].dt.floor('D') # Get just the date (YYYY-MM-DD)
    print("      ‚úÖ 'fecha programada' converted to datetime and date part extracted.")

    # 3. Create the 'campaign_group' column based on 'nombre campa√±a'
    df['nombre campa√±a_lower'] = df['nombre campa√±a'].astype(str).str.lower()
    
    # Define the mapping
    campaign_mapping = {
        'pash': ['pash'],
        'gmac': ['gm', 'insoluto', 'chevrolet'],
        'claro': ['210', '0_', 'rr', 'ascard', 'bscs', 'prechurn', 'churn', 'potencial', 'prepotencial', 'descuento', 'esp', '30_', 'prees', 'preord'],
        'puntored': ['puntored'],
        'crediveci': ['crediveci'],
        'yadinero': ['dinero'],
        'qnt': ['qnt'],
        'habi': ['habi'],
        'payjoy': ['payjoy', 'pay joy']
    }

    df['campaign_group'] = df['nombre campa√±a'] # Default to original name
    for group, keywords in campaign_mapping.items():
        for keyword in keywords:
            # Use contains for partial matches
            df.loc[df['nombre campa√±a_lower'].str.contains(keyword, na=False), 'campaign_group'] = group
    
    print("      ‚úÖ 'campaign_group' column created based on 'nombre campa√±a'.")
    df.drop(columns=['nombre campa√±a_lower'], inplace=True) # Clean up helper column

    # 4. Filter out rows where 'fecha_programada_dia' is NaT (invalid date) before grouping
    df_filtered_for_agg = df.dropna(subset=['fecha_programada_dia'])

    if not df_filtered_for_agg.empty:
        # Group by the date, the campaign group, and the standard/personalizado column
        ivr_saem_aggregated_df = df_filtered_for_agg.groupby(
            ['fecha_programada_dia', 'campaign_group', standard_personalizado_col]
        )['segundos'].sum().reset_index()

        ivr_saem_aggregated_df.rename(
            columns={
                'segundos': 'suma_segundos_diarios',
                standard_personalizado_col: 'tipo_estandar_personalizado' # Rename the unnamed column
            },
            inplace=True
        )
        ivr_saem_aggregated_df['source_file_type'] = 'IVR_SAEM_AGGREGATED'

        print(f"\n   üìà Aggregated IVR SAEM Data: {len(ivr_saem_aggregated_df)} rows")

        print(f"üéâ *** IVR SAEM processing finished. Returning original and aggregated data. ***\n" + "-" * 50)
        return [df, ivr_saem_aggregated_df] # Return a list of DataFrames
    else:
        print("      ‚ö†Ô∏è  No valid data remaining after filtering for aggregation.")
        print(f"üéâ *** IVR SAEM processing finished. Returning original data only. ***\n" + "-" * 50)
        return df # Return original DataFrame if aggregation failed
    
    
def process_ivr_ipcom(file_path, present_headers):
    """
    üìû Logic to process IVR IPCOM files.
    üìà Includes aggregation of 'segundos' by 'fecha programada' (day) and a new
    categorical column derived from 'nombre campa√±a' and the 'Estandar/Personalizado' column.
    """
    print(f"üöÄ *** Starting IVR IPCOM processing for: '{file_path}' ***")
    df = _read_and_normalize_csv_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    df['source_file_type'] = 'IVR_IPCOM' # Mark the original data

    # --- IVR IPCOM SPECIFIC AGGREGATION ---
    required_cols = ['connect time', 'tiempo facturado', 'account name', 'costo']
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"      ‚ö†Ô∏è  Skipping aggregation: Missing one or more required columns: {missing_cols}.")
        print(f"üéâ *** IVR IPCOM processing finished. Returning original data only. ***\n" + "-" * 50)
        return df

    print("   üìä Performing aggregation for 'segundos' by 'fecha programada', 'campaign_group', and 'standard_personalizado_type'...")

    # 1. Convert 'segundos' to numeric, filling NaNs with 0
    df['segundos'] = pd.to_numeric(df['tiempo facturado'], errors='coerce').fillna(0)
    print("      ‚úÖ 'segundos' column converted to numeric and NaNs filled with 0.")
    
    df['costo'] = pd.to_numeric(df['costo'], errors='coerce').fillna(0)
    print("      ‚úÖ 'costo' column converted to numeric and NaNs filled with 0.")

    # 2. Convert 'fecha programada' to datetime and extract the date part
    df['fecha programada'] = pd.to_datetime(df['connect time'], errors='coerce')
    df['fecha_programada_dia'] = df['fecha programada'].dt.floor('D') # Get just the date (YYYY-MM-DD)
    print("      ‚úÖ 'fecha programada' converted to datetime and date part extracted.")

    # 3. Create the 'campaign_group' column based on 'nombre campa√±a'
    df['nombre campa√±a_lower'] = df['account name'].astype(str).str.lower()
    
    # Define the mapping
    campaign_mapping = {
        'pash': ['pash'],
        'gmac': ['gm', 'insoluto', 'chevrolet'],
        'claro': ['210', '0_30', 'rr', 'ascard', 'bscs', 'prechurn', 'churn', 'potencial', 'prepotencial', 'descuento', 'esp', '30_', 'prees', 'preord'],
        'puntored': ['puntored'],
        'crediveci': ['crediveci'],
        'yadinero': ['dinero'],
        'qnt': ['qnt'],
        'habi': ['habi'],
        'payjoy': ['payjoy', 'pay joy']
    }

    df['campaign_group'] = df['account name'] # Default to original name
    for group, keywords in campaign_mapping.items():
        for keyword in keywords:
            # Use contains for partial matches
            df.loc[df['nombre campa√±a_lower'].str.contains(keyword, na=False), 'campaign_group'] = group
    
    print("      ‚úÖ 'campaign_group' column created based on 'nombre campa√±a'.")
    df.drop(columns=['nombre campa√±a_lower'], inplace=True) # Clean up helper column

    # 4. Filter out rows where 'fecha_programada_dia' is NaT (invalid date) before grouping
    df_filtered_for_agg = df.dropna(subset=['fecha_programada_dia'])

    if not df_filtered_for_agg.empty:
        # Group by the date, the campaign group, and the standard/personalizado column
        ivr_ipcom_aggregated_df = df_filtered_for_agg.groupby(
            ['fecha_programada_dia', 'campaign_group'] 
        ).agg(
            
            suma_segundos_diarios=('segundos', 'sum'),
            suma_costo_diario=('costo', 'sum') 
        ).reset_index()

        ivr_ipcom_aggregated_df.rename(
            columns={
                'segundos': 'suma_segundos_diarios',
                'suma_costo_diario': 'tipo_estandar_personalizado' # Rename the unnamed column
            },
            inplace=True
        )
        ivr_ipcom_aggregated_df['source_file_type'] = 'IVR_IPCOM_AGGREGATED'

        print(f"\n   üìà Aggregated IVR IPCOM Data: {len(ivr_ipcom_aggregated_df)} rows")

        print(f"üéâ *** IVR IPCOM processing finished. Returning original and aggregated data. ***\n" + "-" * 50)
        return [df, ivr_ipcom_aggregated_df] # Return a list of DataFrames
    else:
        print("      ‚ö†Ô∏è  No valid data remaining after filtering for aggregation.")
        print(f"üéâ *** IVR IPCOM processing finished. Returning original data only. ***\n" + "-" * 50)
        return df # Return original DataFrame if aggregation failed

def process_email_masivian(file_path, present_headers):
    """
    üìß Logic to process EMAIL MASIVIAN files.
    üìà Includes aggregation of 'procesados' by 'fecha de env√≠o' (day) and 'remitente'.
    """
    print(f"üöÄ *** Starting EMAIL MASIVIAN processing for: '{file_path}' ***")
    df = _read_and_normalize_excel_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    df['source_file_type'] = 'EMAIL_MASIVIAN' # Mark the original data

    # --- EMAIL MASIVIAN SPECIFIC AGGREGATION ---
    required_cols = ['fecha de env√≠o', 'procesados', 'remitente']
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"      ‚ö†Ô∏è  Skipping aggregation: Missing one or more required columns: {missing_cols}.")
        print(f"üéâ *** EMAIL MASIVIAN processing finished. Returning original data only. ***\n" + "-" * 50)
        return df

    print("   üìä Performing aggregation for 'procesados' by 'fecha de env√≠o' and 'remitente'...")

    # 1. Convert 'procesados' to numeric, filling NaNs with 0
    df['procesados'] = pd.to_numeric(df['procesados'], errors='coerce').fillna(0)
    print("      ‚úÖ 'procesados' column converted to numeric and NaNs filled with 0.")

    # 2. Convert 'fecha de env√≠o' to datetime and extract the date part
    df['fecha de env√≠o'] = pd.to_datetime(df['fecha de env√≠o'], errors='coerce')
    df['fecha_envio_dia'] = df['fecha de env√≠o'].dt.floor('D') # Get just the date (YYYY-MM-DD)
    print("      ‚úÖ 'fecha de env√≠o' converted to datetime and date part extracted.")

    # 3. Filter out rows where 'fecha_envio_dia' is NaT (invalid date) before grouping
    df_filtered_for_agg = df.dropna(subset=['fecha_envio_dia'])

    if not df_filtered_for_agg.empty:
        # Group by the date and the remitente and sum 'procesados'
        email_masivian_aggregated_df = df_filtered_for_agg.groupby(
            ['fecha_envio_dia', 'remitente']
        )['procesados'].sum().reset_index()

        email_masivian_aggregated_df.rename(
            columns={'procesados': 'suma_procesados_diarios'},
            inplace=True
        )
        email_masivian_aggregated_df['source_file_type'] = 'EMAIL_MASIVIAN_AGGREGATED'

        print(f"\n   üìà Aggregated EMAIL MASIVIAN Data: {len(email_masivian_aggregated_df)} rows")

        print(f"üéâ *** EMAIL MASIVIAN processing finished. Returning original and aggregated data. ***\n" + "-" * 50)
        return [df, email_masivian_aggregated_df] # Return a list of DataFrames
    else:
        print("      ‚ö†Ô∏è  No valid data remaining after filtering for aggregation.")
        print(f"üéâ *** EMAIL MASIVIAN processing finished. Returning original data only. ***\n" + "-" * 50)
        return df # Return original DataFrame if aggregation failed

def process_sms_masivian(file_path, present_headers):
    """
    üì± Logic to process SMS MASIVIAN files.
    üìà Includes aggregation of 'total procesados' by 'fecha programado' (day)
    and a new categorical column derived from 'campa√±a'.
    """
    print(f"üöÄ *** Starting SMS MASIVIAN processing for: '{file_path}' ***")
    df = _read_and_normalize_excel_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    df['source_file_type'] = 'SMS_MASIVIAN' # Mark the original data

    # --- SMS MASIVIAN SPECIFIC AGGREGATION ---
    required_cols = ['fecha programado', 'total procesados', 'campa√±a']
    
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"      ‚ö†Ô∏è  Skipping aggregation: Missing one or more required columns: {missing_cols}.")
        print(f"üéâ *** SMS MASIVIAN processing finished. Returning original data only. ***\n" + "-" * 50)
        return df

    print("   üìä Performing aggregation for 'total procesados' by 'fecha programado' and 'campaign_group'...")

    # 1. Convert 'total procesados' to numeric, filling NaNs with 0
    df['total procesados'] = pd.to_numeric(df['total procesados'], errors='coerce').fillna(0)
    print("      ‚úÖ 'total procesados' column converted to numeric and NaNs filled with 0.")

    # 2. Convert 'fecha programado' to datetime and extract the date part
    df['fecha programado'] = pd.to_datetime(df['fecha programado'], errors='coerce')
    df['fecha_programado_dia'] = df['fecha programado'].dt.floor('D') # Get just the date (YYYY-MM-DD)
    print("      ‚úÖ 'fecha programado' converted to datetime and date part extracted.")

    # 3. Create the 'campaign_group' column based on 'campa√±a'
    df['campa√±a_lower'] = df['campa√±a'].astype(str).str.lower()
    
    # Define the mapping (provided in the prompt)
    campaign_mapping = {
        'pash': ['pash'],
        'gmac': ['gm', 'insoluto', 'chevrolet'],
        'claro': ['210', '_30', 'rr', 'ascard', 'bscs', 'prechurn', 'churn', 'potencial', 'prepotencial', 'especial'],
        'puntored': ['puntored'],
        'crediveci': ['crediveci'],
        'yadinero': ['dinero'],
        'qnt': ['qnt'],
        'habi': ['habi'],
        'payjoy': ['payjoy', 'pay joy']
    }

    df['campaign_group'] = df['campa√±a'] # Default to original name
    for group, keywords in campaign_mapping.items():
        for keyword in keywords:
            # Use contains for partial matches
            df.loc[df['campa√±a_lower'].str.contains(keyword, na=False), 'campaign_group'] = group
    
    print("      ‚úÖ 'campaign_group' column created based on 'campa√±a'.")
    df.drop(columns=['campa√±a_lower'], inplace=True) # Clean up helper column

    # 4. Filter out rows where 'fecha_programado_dia' is NaT (invalid date) before grouping
    df_filtered_for_agg = df.dropna(subset=['fecha_programado_dia'])

    if not df_filtered_for_agg.empty:
        # Group by the date and the campaign group and sum 'total procesados'
        sms_masivian_aggregated_df = df_filtered_for_agg.groupby(
            ['fecha_programado_dia', 'campaign_group']
        )['total procesados'].sum().reset_index()

        sms_masivian_aggregated_df.rename(
            columns={'total procesados': 'suma_total_procesados_diarios'},
            inplace=True
        )
        sms_masivian_aggregated_df['source_file_type'] = 'SMS_MASIVIAN_AGGREGATED'

        print(f"\n   üìà Aggregated SMS MASIVIAN Data: {len(sms_masivian_aggregated_df)} rows")

        print(f"üéâ *** SMS MASIVIAN processing finished. Returning original and aggregated data. ***\n" + "-" * 50)
        return [df, sms_masivian_aggregated_df] # Return a list of DataFrames
    else:
        print("      ‚ö†Ô∏è  No valid data remaining after filtering for aggregation.")
        print(f"üéâ *** SMS MASIVIAN processing finished. Returning original data only. ***\n" + "-" * 50)
        return df # Return original DataFrame if aggregation failed

def process_wisebot(file_path, present_headers, wisebot_subtype):
    """
    ü§ñ Logic to process WISEBOT files, with sub-classification and specific Wisebot transformations.
    üö´ Filters out 'CONTESTADORA' from ESTADO_LLAMADA, sums TIEMPO_LLAMADA,
    and groups by FECHA_LLAMADA (day) and CAMPA√ëA.
    """
    print(f"üöÄ *** Starting WISEBOT processing ({wisebot_subtype}): '{file_path}' ***")
    df = _read_and_normalize_excel_data(file_path)
    print(f"   üéØ Normalized columns: {len(df.columns)}")

    # --- WISEBOT SPECIFIC LOGIC ---

    # 1. Filter out rows where 'estado_llamada' is 'contestadora'
    df_filtered = df.copy() # Start with a copy to avoid modifying original df
    if 'estado_llamada' in df_filtered.columns and 'marca' not in df_filtered.columns:
        initial_rows = len(df_filtered)
        # Ensure comparison is case-insensitive for 'contestadora'
        df_filtered = df_filtered[df_filtered['estado_llamada'].astype(str).str.lower() != 'contestadora']
        print(f"   üö´ Filtered out 'CONTESTADORA' from 'estado_llamada'. Rows remaining: {len(df_filtered)} (Removed: {initial_rows - len(df_filtered)})")
    else:
        print("   ‚ö†Ô∏è  Warning: 'estado_llamada' column not found for filtering.")

    # 2. Convert 'tiempo_llamada' to numeric, handling errors
    if 'tiempo_llamada' in df_filtered.columns:
        df_filtered['tiempo_llamada'] = pd.to_numeric(df_filtered['tiempo_llamada'], errors='coerce').fillna(0)
        print("   ‚úÖ 'tiempo_llamada' converted to numeric.")
    else:
        print("   ‚ö†Ô∏è  Warning: 'tiempo_llamada' column not found for summation. Adding as 0.")
        df_filtered['tiempo_llamada'] = 0 # Add column with zeros if missing

    # 3. Convert 'fecha_llamada' to datetime and extract date part for grouping
    if 'fecha_estado_final' in df_filtered.columns:
        df_filtered['fecha_estado_final'] = pd.to_datetime(df_filtered['fecha_estado_final'], errors='coerce')
        df_filtered['fecha_dia'] = df_filtered['fecha_estado_final'].dt.floor('D') # Extract date part (YYYY-MM-DD)
        print("   ‚úÖ 'fecha_estado_final' converted to date for grouping.")
    elif 'fecha_llamada' in df_filtered.columns:
        df_filtered['fecha_llamada'] = pd.to_datetime(df_filtered['fecha_llamada'], errors='coerce')
        df_filtered['fecha_dia'] = df_filtered['fecha_llamada'].dt.floor('D') # Extract date part (YYYY-MM-DD)
        print("   ‚úÖ 'fecha_llamada' converted to date for grouping.")
    else:
        print("   ‚ùå Error: 'fecha_estado_final' or 'fecha_llamada' column not found for grouping. Cannot perform aggregation.")
        print(f"üéâ *** WISEBOT processing finished with errors. ***\n" + "-" * 50)
        return None # Return None if key column for grouping is missing

    # Ensure 'campa√±a' column exists for grouping
    if 'campa√±a' not in df_filtered.columns and 'marca' in df_filtered.columns:
        print("   ‚ùå Error: 'campa√±a' column not found for grouping. Cannot perform aggregation.")
        print(f"üéâ *** WISEBOT processing finished with errors. ***\n" + "-" * 50)
        return None # Return None if key column for grouping is missing

    # 4. Group by 'fecha_dia' and 'campa√±a' and sum 'tiempo_llamada'
    # Filter out rows where fecha_dia is NaT (Not a Time) due to parsing errors
    df_filtered = df_filtered.dropna(subset=['fecha_dia'])
    if not df_filtered.empty and 'marca' not in df_filtered.columns:
        wisebot_grouped_df = df_filtered.groupby(['fecha_dia', 'campa√±a'])['tiempo_llamada'].sum().reset_index()
        # Add a source_file_type to the aggregated Wisebot data too
        wisebot_grouped_df['source_file_type'] = f'WISEBOT_{wisebot_subtype.upper()}_AGGREGATED'
        print(f"\n   üìà Aggregated Wisebot Data: {len(wisebot_grouped_df)} rows")
    elif not df_filtered.empty and 'marca' in df_filtered.columns:
        wisebot_grouped_df = df_filtered.groupby(['fecha_dia', 'marca'])['tiempo_llamada'].sum().reset_index()
        # Add a source_file_type to the aggregated Wisebot data too
        wisebot_grouped_df['source_file_type'] = f'WISEBOT_{wisebot_subtype.upper()}_AGGREGATED'
        print(f"\n   üìà Aggregated Wisebot Data: {len(wisebot_grouped_df)} rows")
    else:
        wisebot_grouped_df = pd.DataFrame(columns=['fecha_dia', 'campa√±a', 'tiempo_llamada', 'source_file_type'])
        print("   ‚ö†Ô∏è  No valid data remaining after filtering and date parsing for aggregation.")

    # --- Sub-classification specific details (for logging/debugging) ---
    if wisebot_subtype == "wisebot_benefits":
        print("   üéÅ Wisebot Subtype: With benefits (nombre, apellido, desea_beneficios).")
    elif wisebot_subtype == "wisebot_agreement":
        print("   ü§ù Wisebot Subtype: With agreement and deadline (id base, fecha_acuerdo, fecha_plazo).")
    elif wisebot_subtype == "wisebot_titular":
        print("   üë§ Wisebot Subtype: With titular information (marca).")
    elif wisebot_subtype == "wisebot_base":
        print("   üì¶ Wisebot Subtype: Base (only basic columns).")
    else:
        print("   ‚ùì Wisebot Subtype: Unknown (this should not happen if classification is correct).")

    print(f"üéâ *** WISEBOT processing finished. ***\n" + "-" * 50)
    return wisebot_grouped_df

# --- Modified Save Function to unify columns, filter aggregated, and unify grouping concepts ---
def save_combined_data_to_single_excel_sheet(dataframes_prices, dataframe_registers, output_folder, output_filename="consolidated_data.xlsx"):
    """
    üîó Combines a list of DataFrames into a single DataFrame using an outer join
    üíæ Saves it to one sheet in an Excel file.
    üéØ Filters DataFrames to include only specified aggregated types, unifies
    date, sum, and grouping columns, and then unifies grouping concepts
    within the 'agrupador_campana_usuario' column.

    Args:
        dataframes_prices (list): A list of pandas DataFrames to combine.
        output_folder (str): The directory where the Excel file will be saved.
        output_filename (str): The name of the output Excel file.
    """
    print(f"\nüíæ Starting save process for combined data...")
    print(f"üìÅ Output folder: {output_folder}")
    print(f"üìÑ Output file: {output_filename}")
    
    if not dataframes_prices:
        print("‚ö†Ô∏è No DataFrames to combine. Skipping output file creation.")
        return

    # Define the list of desired aggregated source file types
    desired_aggregated_types = [
        'EMAIL_MASIVIAN_AGGREGATED',
        'SMS_MASIVIAN_AGGREGATED',
        'WISEBOT_WISEBOT_AGREEMENT_AGGREGATED',
        'WISEBOT_WISEBOT_BENEFITS_AGGREGATED',
        'WISEBOT_WISEBOT_BASE_AGGREGATED',
        'WISEBOT_WISEBOT_TITULAR_AGGREGATED',
        'IVR_SAEM_AGGREGATED',
        'SMS_SAEM_AGGREGATED',
        'IVR_IPCOM_AGGREGATED'
    ]

    print(f"üéØ Looking for {len(desired_aggregated_types)} aggregated data types...")

    # Define column unification mappings
    date_columns_map = {
        'fecha_envio_dia': 'fecha_movimiento',
        'fecha_programado_dia': 'fecha_movimiento',
        'fecha_dia': 'fecha_movimiento',
        'fecha_programada_dia': 'fecha_movimiento',
        'fecha_inicio_dia': 'fecha_movimiento'
    }

    sum_columns_map = {
        'suma_procesados_diarios': 'valor_movimiento',
        'suma_total_procesados_diarios': 'valor_movimiento',
        'tiempo_llamada': 'valor_movimiento',
        'suma_segundos_diarios': 'valor_movimiento',
        'suma_ejecutados_diarios': 'valor_movimiento'
    }

    grouping_columns_map = {
        'remitente': 'agrupador_campana_usuario',
        'campaign_group': 'agrupador_campana_usuario',
        'marca': 'agrupador_campana_usuario',
        'nombre de usuario': 'agrupador_campana_usuario',
        'campa√±a': 'agrupador_campana_usuario',
        'username': 'agrupador_campana_usuario'
    }

    # List to hold DataFrames after filtering and renaming
    processed_and_renamed_dfs = []
    found_types = set()

    print(f"\nüîç Processing {len(dataframes_prices)} dataframes...")
    for idx, df in enumerate(dataframes_prices, 1):
        if df is not None and not df.empty:
            if 'source_file_type' in df.columns:
                current_source_type = df['source_file_type'].iloc[0]
                if current_source_type in desired_aggregated_types:
                    print(f"   {idx}. üìä {current_source_type}: {len(df)} rows")
                    df_to_add = df.copy()
                    found_types.add(current_source_type)

                    # Apply renaming for date columns
                    for old_name, new_name in date_columns_map.items():
                        if old_name in df_to_add.columns:
                            df_to_add.rename(columns={old_name: new_name}, inplace=True)
                            df_to_add[new_name] = pd.to_datetime(df_to_add[new_name], errors='coerce').dt.date

                    # Apply renaming for sum columns
                    for old_name, new_name in sum_columns_map.items():
                        if old_name in df_to_add.columns:
                            df_to_add.rename(columns={old_name: new_name}, inplace=True)

                    # Apply renaming for grouping columns
                    marca_renamed = False
                    for old_name, new_name in grouping_columns_map.items():
                        if old_name == "marca" and old_name in df_to_add.columns:
                            df_to_add.rename(columns={old_name: new_name}, inplace=True)
                            marca_renamed = True
                        elif old_name == "nombre de la campa√±a" and old_name in df_to_add.columns:
                            df_to_add.rename(columns={old_name: new_name}, inplace=True)
                            marca_renamed = True
                        elif old_name == "campa√±a" and old_name in df_to_add.columns and marca_renamed:
                            continue
                        elif old_name in df_to_add.columns:
                            df_to_add.rename(columns={old_name: new_name}, inplace=True)
                    
                    processed_and_renamed_dfs.append(df_to_add)
                else:
                    print(f"   {idx}. ‚è≠Ô∏è  Skipping {current_source_type} (not in aggregated types)")
            else:
                print(f"   {idx}. ‚ö†Ô∏è  Skipping dataframe without source_file_type")
        else:
            print(f"   {idx}. ‚ùå Skipping empty or None dataframe")

    if not processed_and_renamed_dfs:
        print("‚ö†Ô∏è No valid or desired aggregated DataFrames found after filtering and renaming. Skipping output file creation.")
        return

    print(f"\n‚úÖ Found {len(processed_and_renamed_dfs)} aggregated dataframes")
    print(f"üìä Found data types: {', '.join(found_types)}")

    # Concatenate all valid and filtered DataFrames into one
    try:
        print(f"\nüîó Concatenating dataframes...")
        combined_df = pd.concat(processed_and_renamed_dfs, ignore_index=True, join='outer')
        print(f"‚úÖ Successfully combined {len(processed_and_renamed_dfs)} filtered and unified DataFrames")
        print(f"üìä Total rows in combined DataFrame: {len(combined_df):,}")
        print(f"üìä Total columns in combined DataFrame: {len(combined_df.columns)}")
    except Exception as e:
        print(f"‚ùå Error concatenating filtered DataFrames: {e}")
        return

    # --- New: Unify concepts in 'agrupador_campana_usuario' ---
    if 'agrupador_campana_usuario' in combined_df.columns:
        print(f"\nüéØ Applying final concept unification to 'agrupador_campana_usuario'...")
        
        initial_unique = combined_df['agrupador_campana_usuario'].nunique()
        print(f"   Initial unique values: {initial_unique}")

        # Create a lowercase version for comparison
        combined_df['agrupador_lower'] = combined_df['agrupador_campana_usuario'].astype(str).str.lower()

        # Apply the unification logic
        # 1. Claro: contains "claro"
        combined_df.loc[combined_df['agrupador_lower'].str.contains('claro', na=False), 'agrupador_campana_usuario'] = 'CLARO'
        
        # 2. Claro: contains "recupera" AND source_file_type is SMS_SAEM_AGGREGATED
        combined_df.loc[
            (combined_df['agrupador_lower'].str.contains('recupera', na=False)) &
            (combined_df['source_file_type'] == 'SMS_SAEM_AGGREGATED'),
            'agrupador_campana_usuario'
        ] = 'CLARO'

        # 3. Gm Financial: contains "chevrolet", "gm", or "insoluto"
        combined_df.loc[combined_df['agrupador_lower'].str.contains('chevrolet|gm|insoluto', na=False), 'agrupador_campana_usuario'] = 'GMAC'

        # 4. qnt
        combined_df.loc[combined_df['agrupador_lower'].str.contains('qnt', na=False), 'agrupador_campana_usuario'] = 'QNT'
        
        # 5. yadinero
        combined_df.loc[combined_df['agrupador_lower'].str.contains('dinero', na=False), 'agrupador_campana_usuario'] = 'YA DINERO'

        # 6. pash
        combined_df.loc[combined_df['agrupador_lower'].str.contains('pash|credito', na=False), 'agrupador_campana_usuario'] = 'PASH'

        # 7. puntored
        combined_df.loc[combined_df['agrupador_lower'].str.contains('puntored', na=False), 'agrupador_campana_usuario'] = 'PUNTORED'
        
        # 8. habi
        combined_df.loc[combined_df['agrupador_lower'].str.contains('habi', na=False), 'agrupador_campana_usuario'] = 'HABI'
        
        # 9. crediveci
        combined_df.loc[combined_df['agrupador_lower'].str.contains('crediveci', na=False), 'agrupador_campana_usuario'] = 'CREDIVECI'
        
        # 10. payjoy
        combined_df.loc[combined_df['agrupador_lower'].str.contains('payjoy', na=False), 'agrupador_campana_usuario'] = 'PAYJOY'

        # Drop the temporary lowercase column
        combined_df.drop(columns=['agrupador_lower'], inplace=True)
        
        final_unique = combined_df['agrupador_campana_usuario'].nunique()
        print(f"   ‚úÖ Final unique values: {final_unique}")
        print(f"   üìâ Reduced by: {initial_unique - final_unique} unique values")
    else:
        print("‚ö†Ô∏è Warning: 'agrupador_campana_usuario' column not found for final concept unification.")

    os.makedirs(output_folder, exist_ok=True) # Ensure output folder exists
    output_filepath = os.path.join(output_folder, output_filename)

    try:
        print(f"\nüíæ Saving to Excel file...")
        # Save both DataFrames to different sheets in the same Excel file
        with pd.ExcelWriter(output_filepath, engine='openpyxl') as writer:
            combined_df.to_excel(writer, sheet_name='Data Faturada', index=False)
            dataframe_registers.to_excel(writer, sheet_name='Data Campana', index=False)
        
        print(f"üéâ ‚úÖ SUCCESS!")
        print(f"üìä File saved: '{output_filepath}'")
        print(f"   üìÑ 'Data Faturada' sheet: {len(combined_df):,} rows")
        print(f"   üìÑ 'Data Campana' sheet: {len(dataframe_registers):,} rows")
        print(f"   ‚è∞ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"‚ùå Error saving processed data to Excel: {e}")
    
# --- Step 5: Main Orchestration Function (Corrected) ---
def process_excel_files_in_folder(input_folder, output_folder):
    """
    üîÑ Iterates through Excel files in an input folder, classifies them,
    üîß applies the corresponding processing function, and collects results
    üíæ to be saved into a single sheet of an output Excel file.

    Args:
        input_folder (str): The path to the folder containing Excel files to process.
        output_folder (str): The path to the folder where processed data will be saved.
    """
    print(f"\n{'='*60}")
    print(f"üöÄ STARTING TELECOMMUNICATIONS DATA PROCESSING")
    print(f"{'='*60}")
    print(f"üìÇ Input folder: {input_folder}")
    print(f"üíæ Output folder: {output_folder}")
    
    if not os.path.exists(input_folder):
        print(f"‚ùå Error: Input folder '{input_folder}' does not exist.")
        return

    dataframes_prices = [] # List to store all processed DataFrames
    file_count = 0
    processed_count = 0

    print(f"\nüîç Scanning for Excel/CSV files...")
    for filename in os.listdir(input_folder):
        if filename.endswith((".xlsx", ".xls", ".csv")): # Check for Excel files
            file_count += 1
            file_path = os.path.join(input_folder, filename)
            print(f"\n{'‚îÄ'*40}")
            print(f"üìÑ [{file_count}] Processing: {filename}")
            print(f"{'‚îÄ'*40}")

            file_type, present_headers = classify_excel_file(file_path)
            processed_data = None # Initialize processed_data for each file

            if file_type == "sms_saem":
                processed_data = process_sms_saem(file_path, present_headers)
            elif file_type == "ivr_saem":
                processed_data = process_ivr_saem(file_path, present_headers)
            elif file_type == "ivr_ipcom":
                processed_data = process_ivr_ipcom(file_path, present_headers)
            elif file_type == "email_masivian":
                processed_data = process_email_masivian(file_path, present_headers)
            elif file_type == "sms_masivian":
                processed_data = process_sms_masivian(file_path, present_headers)
            elif file_type.startswith("wisebot"):
                processed_data = process_wisebot(file_path, present_headers, file_type)
            elif file_type == "unknown":
                print(f"‚ö†Ô∏è  ATTENTION: Unknown file type for '{filename}'. No specific processing applied.")
                print(f"   üìä Columns found: {len(present_headers)}")
                continue # Skip to next file
            elif file_type.startswith("error"):
                print(f"‚ùå Could not process '{filename}' due to an error during classification.")
                continue # Skip to next file
            else:
                print(f"‚ùå Internal error: Unexpected classification '{file_type}' for '{filename}'.")
                continue # Skip to next file

            # Handle the processed_data: if it's a list, extend the main list; otherwise, append it.
            if processed_data is not None:
                if isinstance(processed_data, list):
                    # If it's a list of DataFrames, extend the main list
                    for df_item in processed_data:
                        if df_item is not None and not df_item.empty:
                            dataframes_prices.append(df_item)
                            processed_count += 1
                            print(f"   ‚úÖ Successfully collected a DataFrame from '{filename}'")
                        elif df_item is not None and df_item.empty:
                            print(f"   ‚ö†Ô∏è  A DataFrame processed from '{filename}' resulted in an empty DataFrame.")
                        else:
                            print(f"   ‚ùå A DataFrame processed from '{filename}' was None.")
                    print(f"   üìä All DataFrames from '{filename}' handled.")
                elif not processed_data.empty:
                    # If it's a single DataFrame, append it
                    dataframes_prices.append(processed_data)
                    processed_count += 1
                    print(f"   ‚úÖ Successfully processed '{filename}'.")
                else:
                    print(f"   ‚ö†Ô∏è  Processed '{filename}' resulted in an empty DataFrame.")
            else:
                 print(f"   ‚ùå Processing of '{filename}' failed or returned None.")

    print(f"\n{'='*60}")
    print(f"üìä PROCESSING SUMMARY")
    print(f"{'='*60}")
    print(f"üìÇ Total files found: {file_count}")
    print(f"‚úÖ Successfully processed: {processed_count}")
    print(f"üìä Total dataframes collected: {len(dataframes_prices)}")
    
    # Process registers data
    print(f"\nüìä Processing campaign registers data...")
    dataframe_registers = registers_telematic.process_excel_files_in_folder(input_folder)
    
    if dataframe_registers is not None and not dataframe_registers.empty:
        print(f"‚úÖ Campaign data processed: {len(dataframe_registers)} rows")
    else:
        print(f"‚ö†Ô∏è  No campaign register data found")
    
    # Save all collected DataFrames to a single Excel sheet
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_excel_filename = f"data_consolidada_telematica_{timestamp}.xlsx"
    save_combined_data_to_single_excel_sheet(dataframes_prices, dataframe_registers, output_folder, output_excel_filename)
    
    print(f"\n{'='*60}")
    print(f"üéâ PROCESSING COMPLETED")
    print(f"{'='*60}")
    print(f"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üíæ Output file: {output_excel_filename}")